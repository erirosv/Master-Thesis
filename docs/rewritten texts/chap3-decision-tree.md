A Decision Tree is a hierarchical tree structure that is commonly used for classification and regression problems. It recursively splits the data into subsets based on the values of the features in the dataset. The splits are chosen based on the feature that provides the most information gain, calculated using metrics such as the Gini impurity or entropy [42], [43].

Information gain is a measure of the reduction in uncertainty that is achieved by splitting the data based on a particular feature. The Gini impurity, on the other hand, is a measure of how often a randomly chosen element from the set would be incorrectly labelled if it was randomly labelled according to the distribution of labels in the subset. To split the data, the Decision Tree algorithm considers all possible splits for each feature and chooses the one that maximizes the information gain. This process is repeated recursively until a stopping criterion is met, such as a maximum depth or a minimum number of samples in a leaf node [42], [43].

**MATH FORMULA FOR INFORMATION GAIN AND PARENT ENTROPY**

To calculate the information gain for a split, the algorithm subtracts the weighted average of child entropies from the parent entropy. The parent entropy is calculated by summing the negative product of the proportion of each class label and the logarithm of the proportion (in base 2) for all the labels in the parent node. The child entropies are calculated similarly, but weighted by the proportion of samples in each child node. The Gini impurity can also be calculated using a similar method, with the entropy term replaced by the Gini impurity [42], [43].

Here is the entire algorithm for Decision Trees: the algorithm recursively splits the data into subsets based on the most informative feature, chosen using information gain calculated using metrics such as the Gini impurity or entropy. The algorithm considers all possible splits for each feature and chooses the one that maximizes the information gain, repeating the process recursively until a stopping criterion is met, such as a maximum depth or a minimum number of samples in a leaf node. To calculate the information gain for a split, the algorithm subtracts the weighted average of child entropies from the parent entropy, with both the parent and child entropies calculated using the proportion of samples in each class label. The resulting tree can be used for both classification and regression problems.

**ALGORITHM**

**IGNORE SUMMARY**
In summary, Decision Trees are a powerful and intuitive algorithm for solving classification and regression problems. They recursively split the data based on the most informative features, with information gain calculated using metrics such as the Gini impurity or entropy. The algorithm considers all possible splits for each feature and chooses the one that maximizes the information gain, and it can be trained using various stopping criteria, such as a maximum depth or a minimum number of samples in a leaf node.
