---
title= "Related Works"
author = "Erik Rosvall"
---
# An efficient feature selection framework based on information theory for high dimensional data

In this section, a brief literature review pertaining to various feature selection methods both that are based on information the- ory and that do not rely on information theory (non-information theory based) is concisely presented. In general, feature selection techniques aim to select a subset of important and informative features from the original feature set, such that the irrelevant and redundant features are discarded, with the view to achieve good performance. Many feature selection techniques that are grounded on information theory have been proposed [53] to select predominant features from high dimensional data. Further, it is crucial to evaluate and measure the relevance between the features and the class as well as the redundancy among the features. Therefore, it is vital to find and define the evaluation criteria adopted to select the important features and eliminate irrelevant and redundant features. Various evaluation criteria are used for selecting predominant features from high dimensional datasets. Information theory based evaluation techniques have been widely used in filter-based feature selection approaches. However, Correlation based Feature Selection and Relief feature selection (commonly used feature reduction techniques) do not rely on information theory based techniques. Correlation based Feature Selection (CFS) [54] is one of the filter-based feature selection methods rooted on the test theory. It evaluates the adequacy of the feature subset by assessing the correlation among the features and between feature and class. Relief algorithm [55], another filter based approach, measures the relevance of the features and the class by sampling the instances from the training dataset randomly. Further, for each feature, it updates the rele- vance score based on the difference among the instances selected and the two nearest instances of different and same class. This algorithm scales well for high dimensional data whereas it does not remove redundant features.
Further, the focus is placed on information theory based meth- ods for selecting predominant features from high dimensional data. In this literature review, fk denotes the candidate feature, fj indicates the selected feature, S represents the selected subset and Y signifies the class. In information theory based methods, Mutual information maximization [56], also known as informa- tion gain (IG), is one of the straight forward feature selection strategies that evaluates the correlation between the features and class by applying the mutual information method. However, this IG technique does not consider the redundant information among the candidate features and the selected features subset.

Recently, a feature selection algorithm has been introduced for selecting the features based on uncertainty change ratio (UCRFS) [62] on the high dimensional datasets. In this technique, the difference between the reduced uncertainty and remained uncer- tainty of the class from the already selected features and candi- date features are considered in a different manner in order to se- lect the features with less redundancy and relevance. The method uses uncertainty change ratio with traditional feature relevance and redundant terms to evaluate candidate features. Further- more, another feature selection technique called dynamic feature importance (DFIFS) [63], which assesses the feature importance and feature redundancy, has been reported. In this method, Gini importance (GI) method that is employed in random forest is used in order to compute feature importance. In order to assess the feature redundancy, maximum information coefficient (MIC) is used. Moreover, this technique also incorporates a few existing filters with the view to achieve high accuracy with less number of features in originally high dimensional datasets.
In the context of microarray gene selection, in order to im- prove the gene selection and the performance effectively with low complexity, a multivariate feature ranking method has been put forward [64]. In this technique, Markov blanket (MB), which simultaneously considers both redundancy among the features and relevance with the class labels, is used for selecting sig- nificant features from high dimensional datasets. This method mainly focuses on the prediction accuracy rather than the inter- pretability of important features.
In this work, information theory based methods such as CMI, CMIM, JMI, DISR, mRMR and methods that are not based on information theory such as CFS and Relief are considered for com- paring and justifying the performance of the proposed feature selection method. The following section provides an account on the background needed for the proposed work.
