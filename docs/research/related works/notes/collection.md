---
title="Collection Of Related Works"
---

## An Efficient SVM-Based Feature Selection Model for Cancer Classification Using High-Dimensional Microarray Data
In recent years, many significant research efforts have been produced to study the cancer microarray data classification using different feature selection techniques, with feature selection playing an important role in cancer classification. As a context for the research discussed in the paper, we pro- vide an overview of this work. Table 1 summarizes some of the previous research methods for microarray cancer classifi- cation.

Cancer classification accuracy is considered in all these previous studies without disclosing biological information on the cancer classification process. The SVM-mRMRe model aims to close the gap between the classification and biological interpretation of cancer by improving accuracy and selecting significant genes that agree with pertinent biomedical studies.

## Genetic algorithm based cancerous gene identification from microarray data using ensemble of filter methods
A brief overview of some recent works on FS in microarray datasets is presented in this section. We have also highlighted the wide use and applicability of the 2-stage hybrid model. Work reported in [21] has proposed an ensemble approach where genes are ranked by using blogreg, t test, and Fisher, and then the sum of the three rankings is used as the final ranking. Then, 1% of the top genes is selected for the next step. Discriminant independent component analysis (dICA) is subsequently used on the selected features to transform them with PSO as the optimization function and SVM as a classifi- er. Another method using PSO is [22], where first correlation- based feature selection (CFS) is used to select discriminative features and Taguchi Chaotic Binary PSO (TCBPSO) is ap- plied on the selected features. K-NN classifier is used with leave-one-out-cross-validation (LOOCV) as fitness measure. Taguchi is implemented using signal-to-noise-ratio (SNR) and orthogonal array (OA). They have proposed a method [23] in which initial datasets are pre-processed using a quartile-based technique and after that Hamming distance-based binary PSO (HDBPSO) is applied. Hamming distance is introduced as a proximity measure to update the velocity of particles in PSO.

In distributed FS, proposed in [24], a dataset is divided into several smaller disjoint sets either randomly or using a ranking method and placing features, with similar relevance to the class, together. Filter method is applied to each subset to select a smaller subset of features from each set. The subsets are joined in a method akin to forward FS (except subsets are used instead of features) to form the final selected subset.

A 2-stage approach proposed in [25] consists of feature ranking using information gain (IG) and then applying binary differential evolution (BDE). Another 2-stage approach pro- posed in [26] consists of five filters—IG, ReliefF, CFS, INERACT, and consistency-based filter. Then, the five sets are fed to a specific classifier whose decisions are combined using simple voting. In [27] also, a 2-stage model is proposed with two filter methods—F score and IG in the first stage. The wrapper method—sequential floating search method (SFSM is a combination of sequential forward selection (SFS) and sequential backward selection (SBS)) starts with the intersection of two filter methods as the initial stage and limits the search space to the union of the two filter method’s results.

In sequential random K-nearest neighbors (SRKNN) [28], authors have taken k-base classifiers, each selecting a set of features using forward sequential selection. Thereafter, the feature sets are combined and if the accuracy is better, the new set is chosen. The process is iterated considering the
output feature set as the new input set for the next iteration. In [29], two variations of kernel ridge regression (KRR), namely, wavelet kernel ridge regression (WKRR) and radial basis kernel ridge regression (RKRR) are used to classify the features obtained from modified cat swarm optimization
(MCSO) using K-NN classifier.

In [30], authors have proposed a quadratic programming
feature selection (QPFS) method based on semidefinite pro- gramming model which is relaxed from the quadratic pro- gramming model with maximizing feature relevance and min- imizing feature redundancy. Lagrange multiplier has been used as proxy measurement to identify the expected features instead of solving a feasible solution for the original max-cut problem.

Variable neighborhood search-based on predominant grouping (PGVNS) [31] consists of variable neighborhood search (VNS), a filter method, and the concept of Markov blankets to group the input space into subsets of features called predominant group. Each predominant group is com- posed of one predominant feature X along with all redundant features for which X forms a Markov blanket.

## MaskedPainter: Feature selection for microarray data analysis
Feature selection is a fundamental task in the bioinformatics domain to identify the most relevant genes correlated with the considered sample classes. In literature many studies have been addressed to this issue. However, evaluating feature selection techniques and comparing results among different works is very difficult due to the lack of both standard experimental designs and benchmark datasets containing a sufficient number of relevant genes known in biological literature [49]. A currently adopted way to evaluate feature selection methods is to use as score the accuracy of a classifier applied after the feature selection. The higher is the accuracy, the more relevant the selected genes are. For example, the authors [24] analyze several feature selection methods available in the RankGene software [40] and show that the choice of feature selection criteria can have a significant impact on classification accuracy.

The feature selection methods can be categorized in three categories: (i) filter, (ii) wrapper and (iii) embedded. In the following we describe the main characteristics of these approaches.

Filter methods use general characteristics of the data to to detect differentially expressed genes. A gene is differentially expressed if it shows a certain distribution of expression levels under one condition and a significantly different distribution under other conditions. Filter methods aim at evaluating the differential expression of genes and rank them according to their ability to distinguish among classes. For example, a P-metric correlation [18] which measures the difference between the samples relative to the standard deviation of samples was exploited. Moreover, these approaches operate independently of any learning algorithm and require less computation. We can identify two main sub-categories of filter methods: (i) univariate and (ii) multivariate.

The univariate approaches are based usually on statistical measures used for detecting differences between two groups (including t-test, Wilcoxon test, and Mann-Whitney test), or among three or more groups (ANOVA, Kruskal-Wallis test, and Friedman test) [27,44]. These methods are easily and very efficiently computed but the disadvantage is that they require some assumptions on the data distribution. For example, the t-test requires expression levels to be normally distributed and homogeneous within groups and may also require equal variances between groups. A Bayesian version of t-test by means of a derivation of point estimates for parameters and hyperparameters was proposed in [14].

Instead, the multivariate approaches are devoted to evaluates the correlations among genes belonging to the selected subset. For example, in [33] a k-means clustering is applied to partition the initial set of genes. A filter approach based on the normalized mutual information metric is conducted in each cluster. A sequential forward search in the space of subsets of genes is performed until a predefined number of genes is achieved. Other approaches are based on searching procedures to identify the best subset of genes. In [37] a multivariate score formulated by the CFS algorithm is integrated as evaluation function of a genetic algorithm.

Wrapper methods evaluate the usefulness of a gene by estimating the accuracy of the learning method applied only to selected genes and not to the entire dataset. The aim of this kind of analysis is to select the features that optimize the performance of the target classifier. It is computationally very expensive for data with a large number of features and the selected subset is dependent on the considered learning algorithm.

Among the feature selection categories, the wrapper methods typically require extensive computation to search the best features and depend on the considered learning algorithm [25]. For example, the authors [32] proposed an approach based on genetic algorithms. The Silhouette statistic is used to assign a score to each subset. In order to reduce the search space, a pre-selection of genes is done by the BSS/WSS univariate filter metric [13]. Moreover, wrapper approaches usually require a further step to avoid the exhaustive search among all the possible solutions, because the number of feature subsets grows exponentially with the number of features, making enumerative search unfeasible. In [35] an univariate rank is first computed for all the genes, and then this ranked list is crossed by a wrapper procedure which incrementally augments the subset of selected genes.

The embedded approaches have the advantage of including the interaction with the classification model, while at the same time being far less computationally intensive than wrapper methods. For example,in [19] the features are ranked with the magnitude of the weights in the SVM classifier. The relevance of each feature is assessed by the calculation of the resubstitution error during a recursive feature elimination procedure. In [15] a variation of this method was proposed. The entropy values of the SVM weights are discretized to eliminate chunks of irrelevant genes. Differently, in [11] the ensemble nature of the decision trees constructed in the random forest framework is exploited to compute the relevance of each feature.

A particular issue of feature selection task is finding the optimal number of genes which improve classification accuracy and show high correlation with disease outcomes. For example, for a two-class cancer subtype classification problem, few tens of genes are usually sufficient, even if there are studies which suggest that one or two genes may be enough [31]. The authors [45] test the classification capability of all simple combinations of the top genes. Firstly, they classify the data set with only one top gene. If the accuracy is not sufficient, they consider all 2-gene combinations of top genes, and consider an increasing number of genes until a good accuracy is reached. High accuracy values are provided by only three or four genes.

For a comparison of various feature selection methods on different microarray data see [16,22,29,42]


## On the Feature Selection of Microarray Data for Cancer Detection based on Random Forest Classifier
> Sort of related work. References to other works

In 2007, Somnath. D and Susmita. D [6] proved that the LASSO (Least Absolute Shrinkage and Selection Operator) method is more effective than the PLS (Partial Least Squares) method because lung cancer data have a higher percentage. The approach used between LASSO and PLS is average imputation to predict the survival time of someone who has cancer. In 2008, Adiwijaya et al. [17] conducted research on the Analysis and Implementation of the Minimum- Redundancy-Maximum-Relevance (MRMR) Feature Selection on the Naïve Bayes classification method. The selected selection feature aims to reduce the size of the data but still produce a good accuracy value. It is not too large if there is a decrease in the accuracy of the data classification [18,19].

In 2010, Li et al. BMC Bioinformatics [8] researched on "classification of G-protein coupled receptors based on support vector machines with maximum relevance of minimum redundancy and genetic algorithms." This study uses the Support Vector Machine (SVM) classification method and dimension reduction as a selection feature using the Minimum Redundancy Maximum Relevance (MRMR) method to predict and classify GPCR directly from amino acid sequence data. A Genetic Algorithm (GA) is used to find the optimal feature subset. The accuracy of three- layer predictors: the superfamily, family, and subfamily levels. Those are obtained from the Cross-Validation test on two non-redundant datasets. The yield can be around 0.5% to 16% higher than GPCR-CA and GPCRPred.

This research differs from previous research studies for the research [4] feature selection of Minimum Redundancy Maximum Relevance only improves the accuracy of lung cancer data. Therefore, this research will use MRMR to enhance the accuracy of the other datasets. In a research [6], LASSO has higher than PLS presentation in the lung cancer data. In research [3], Random Forest has not sufficiently increased if with the Relief Method selection feature. Therefore, this research will use the Random Forest classification with MRMR and LASSO selection features to improve accuracy in other datasets.

## TotalPLS: Local Dimension Reduction for Multicategory Microarray Data
Here, we use the terms “feature selection” and “gene selec- tion” interchangeably. In addition, we focus on analyzing high- dimensional multicategory problems where “high-dimensional datasets” refer to datasets with more than 10 000 dimensions. Numerous real datasets are high-dimensional, multicategory, and nonlinear. Effective analysis of such data is a research priority in machine learning, multivariate data analysis, and data processing in cognitive science. The purpose of dimen- sion reduction is to find some form of structure hiding in the original high-dimensional data. Traditional dimension reduc- tion methods can be generally classified into feature extraction (or feature transformation) and feature selection (or variables selection) [14], [15].

Feature selection algorithms use some statistical indicators or separability criteria to select the most representative features from the original feature sets. The goal is to get an informative feature subset that has a small number of features but contains strong identification information. Usually, two types of methods can be used in feature selection: filter and wrapper methods. A filter method is a single feature-scoring method based on cer- tain criteria, which usually selects features with high scores for further analysis. The most popular filter methods involve using a t-test statistic or F-test statistic (see [16] and [17]), analyzing signal-to-noise ratio (see [18]–[20]), performing a nonparamet- ric test such as the Wilcoxon rank sum test or Kruskal-Wallis rank sum test (see [21], [22]), studying mutual information (see [23]–[25]), implementing a Relief algorithm (see [26]), or a variation of a Relief algorithm in which the evaluation function is not related to the classifier (see [27]). A wrapper method is a feature-selection associated with a classifier. The output of the classifier is treated as a feature-selection criterion. Wrapper methods include Genetic Algorithms (see [28]), SVM (see [6], [29], [30]), the RFE method, and Boosting. A wrapper method usually uses the error probability of a classifier as the evaluation function. In filter method, each feature is evaluated depending only on its inherent self-information and the feature evaluation is not related with the information of other features. The advantage of a filter method is that it is fast and universal (independent of other features and classifier). Its disadvantage is that it ignores the relationship among features. A more ac- curate approach needs to consider the joint distribution among features. It must take into account all the features and should allow for the detection of features that have a relatively small main effect but has at least one strong interaction effect [31]. Those features with a smaller main effect may contain more important information for the studied issues and the analysis of these features may provide more comprehensive understanding of expression patterns for the issues studied.

Good feature selection methods should perform the following functions [32]:
1) take full account of the interaction among features;
2) performance should be based on the feature subset rather
than individual features associated with classification;
3) the feature selection algorithm should be reasonable and efficient and the selected subset should contain fewer fea-
tures than in the initial set;
4) detect features that have a relatively small main effect but
have a strong interaction effect [31].

Therefore, feature selection can be divided into single-feature
selection (univariate method) and multifeature selection (mul- tivariate method) based on whether the approach considers interaction among features or not. SVM-based SVMRFE al- gorithms [6] and Relief algorithms [26] based on an iteratively adjusted margin are currently known to be the best methods for multifeature selection. However, the original two methods ap- ply to two-category classification problems. ReliefF, which is an improved version of the Relief algorithm (see [27]), can solve multicategory classification and regression problems. OVO- and OVA-based multiclass SVM-RFE (see [33]) and the extensions of SVM-RFE are used in multiclass gene selection [10]. These algorithms consider the correlation among features to some ex- tent. Feature selection based on mutual information in informa- tion theoretic learning has been extensively studied [25]. There are many approaches to offset the impact of interaction among features [34]–[36]. The limitation of these methods is the high- computational cost. In addition, most of the algorithms based on information measurements can be unified to a general criterion function about mutual information in feature selection [37].

Feature extraction is a mapping process of the original feature space. It projects the original feature space into low-dimensional space that better reflects the structure information of the orig- inal sample data [15], [38]. There are several commonly used feature extraction methods including principal component anal- ysis (PCA) (see [39]), independent component analysis (ICA) (see [40]), linear discriminant analysis (LDA) (see [39]), and between-group analysis (BGA) (see [41]). They achieve very good results for most datasets with a linear structure. Using the idea of largest variance and minimum deviation, PCA discovers the main direction of the dataset to achieve dimension reduction. However, PCA has limitations when datasets are highly non- linear. ICA assumes the existence of inherent variables in the dataset and uses blind source analysis of information theory to obtain the synthesis features. It does not consider the global and local nature of the observation space of the data. LDA and BGA are supervised feature extraction methods and take into account the a priori information of categories. In addition, the meth- ods based on space-frequency transformation including Fourier transform (FT), discrete cosine transform, Hadamard transform, and wavelet analysis such as Gabor wavelet are generally used in image processing [42]. However, wavelet transforms, FTs, and other orthogonal transforms essentially transform the spa- tial datasets into the frequency domain to achieve dimension reduction. This process does not have an intuitive geometric interpretation. Yan et al. proposed the incremental orthogonal centroid algorithm, which is based on the orthogonal centroid algorithm [15], and Li et al. proposed the maximum margin cri- terion method [43]. These algorithms have good performance.

There are two major research directions regarding model se- lection on high-dimensional datasets. One is based on a regular- ization method in which shrinkage estimates are obtained using a penalty function. Examples include ridge estimation (see [44], [45]); Lasso (see [46]), Least Angle Regression (LARS, see [1]), and Elastic net (see [47]). The other direction is based on com- ponents such as PCA and PLS [48]. The PLS-based supervised feature extraction method can achieve good results with many public databases and benchmark tests. PLS is a nonparametric method based on the idea of high-dimensional projection, and it has been extensively used in high-dimensional microarray data analysis. Nguyen and Rocke [49] and Dai et al. [18] applied PLS in a dimension reduction method called PLSDR for high- dimensional microarray data analysis. Boulesteix et al. system- atically compared several of the most popular PLS approaches and their applications in the fields of tumor classification, iden- tification of relevant genes [50], [51]. Ji et al. proposed PLS- based gene selection to identify tumor-specific genes in two- category and multicategory tumor subtypes classifications [31]. Yang et al. proposed information fusion of PLS-based feature selection and SVM-based classification to predict business fail- ure [52]. Gutkin et al. presented a PLS-based feature selec- tion that dealt with two-category classification [53]. Cao et al. presented a computational methodology called sparse PLS to perform variable selection in a multiclass classification frame- work [54]. Chakraborty and Dutta proposed SVA-PLS method to excavate the hidden sources of expression heterogeneity in gene expression studies [55]. Zhao et al. developed higher-order partial least squares aiming to predict a tensor (multiway array) Y from a tensor X through projecting the data onto the latent space and performing regression on the corresponding latent variables [56]. All indicate that PLS has good performance on dimension reduction and classification problems.

## A review of microarray datasets and applied feature selection methods
> Missing links to source code and pseudocode for any algorithm

Several studies have shown that most genes measured in a DNA microarray experiment are not relevant in the accurate classification of different classes of the problem [46]. To avoid the problem of the ‘‘curse of dimensionality’’ [62], feature(gene) selection plays a crucial role in DNA microarray analysis, which is defined as the process of identifying and removing irrelevant features from the training data, so that the learning algorithm focuses only on those aspects of the training data useful for analysis and future prediction [50]. There are usually three varieties of feature selection methods: filters, wrappers and embedded methods. While wrapper models involve optimizing a predictor as part of the selection process, filter models rely on the general characteristics of the training data to select features independent of any predictor. The embedded meth- ods generally use machine learning models for classification, and then an optimal subset of features is built by the classifier algorithm. Of course, the interaction with the classifier required by wrapper and embedded methods comes with an impor- tant computational burden (more important in the case of wrappers). In addition to this classification, feature selection methods may also be divided into univariate and multivariate types. Univariate methods consider each feature indepen- dently of other features, a drawback that can be overcome by multivariate techniques that incorporate feature dependencies to some degree, at the cost of demanding more computational resources [26].
Feature selection as a preprocessing step to tackle microarray data has rapidly become indispensable among researchers, not only to remove redundant and irrelevant features, but also to help biologists identify the underlying mechanism that relates gene expression to diseases. This research area has received significant attention in recent years (most of the work has been published in the last decade), and new algorithms have emerged as alternatives to the existing ones. However, when a new method is proposed, there is a lack of standard state-of-the-art results to perform a fair comparative study. Fur- thermore, there is a broad suite of microarray datasets to be used in the experiments, some of which even have the same name, but the number of samples or characteristics are different in different studies, which makes this task more complicated.
